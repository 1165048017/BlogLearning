Kullback–Leibler divergence(相对熵，KL距离，KL散度):[理论](https://mp.weixin.qq.com/s?__biz=MzU1NzU2MzcyMw==&mid=2247483828&idx=1&sn=df3a8841d235b0d44b372e0e74a9e0eb&chksm=fc32ab90cb4522865a96bab7f271da6d6d373f1430afa376e5f414c3bd7e46851ee70430f0d0&token=790768732&lang=zh_CN#rd)   
线性回归:[理论](https://mp.weixin.qq.com/s?__biz=MzU1NzU2MzcyMw==&mid=2247483846&idx=1&sn=b1aa2ab4165cce3a80eb399a6b7b0fbe&chksm=fc32abe2cb4522f41fba3920c8f7f1bfafea55274081983c868cf7156e604cb81490f092ea06&token=790768732&lang=zh_CN#rd),[代码](https://github.com/1165048017/BlogLearning/tree/master/Linear%20regression)  
logistic回归:[理论](https://mp.weixin.qq.com/s?__biz=MzU1NzU2MzcyMw==&mid=2247483852&idx=1&sn=be84a7e08e881c98d21aad15d9e2ba7d&chksm=fc32abe8cb4522fe1a4c6f93aad3d9c307a16fb6db3519bfea8ab03dbb5e94e35ce0f656e9ee&token=790768732&lang=zh_CN#rd),[代码](https://github.com/1165048017/BlogLearning/tree/master/logistic%20regression)  
PCA、SVD、ZCA白化:[理论](https://mp.weixin.qq.com/s?__biz=MzU1NzU2MzcyMw==&mid=2247483897&idx=1&sn=335fee084e249a870844f7da86a6fabc&chksm=fc32abddcb4522cbd0d88e307cf775552f143231b6f6a14bd69ba6a25839bd997e1ac4941d7f&token=790768732&lang=zh_CN#rd),[代码](https://github.com/1165048017/BlogLearning/tree/master/PCA)  
K-means理论与实现:[理论](https://mp.weixin.qq.com/s?__biz=MzU1NzU2MzcyMw==&mid=2247483919&idx=1&sn=25db17fb4eaf2334448a166683ea17c0&chksm=fc32a82bcb45213d1b2a56b9b2726d4c79b90f1cb2d95a58f31e894d3d8bc9711f04fa37ef2d&token=790768732&lang=zh_CN#rd),[代码](https://github.com/1165048017/BlogLearning/tree/master/k-means)  
损失函数梯度对比-均方差和交叉熵:[理论](https://mp.weixin.qq.com/s?__biz=MzU1NzU2MzcyMw==&mid=2247483929&idx=1&sn=0350b693de45c736587e6bd290e694cc&chksm=fc32a83dcb45212b2040529e5b0dc7773761889e14f2e4959580a6242b13cbe7778659674c21&token=790768732&lang=zh_CN#rd)  